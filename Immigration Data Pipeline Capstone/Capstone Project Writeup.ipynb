{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration Summary Datasets\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This Project gathers data from multiple source to create a data model which can show the details of where people immigrated to the US in 2016 and the details of those areas\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions import upper, col, isnan, when, count, col, round\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "from pyspark.sql.functions import udf, date_format, split, monotonically_increasing_id\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_df(df):\n",
    "    print('NaNs:\\n')\n",
    "    df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "    print('nulls:\\n')\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    print( 'duplicates: ',df.count()-df.distinct().count())\n",
    "    print(df.count())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['default']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['default']['AWS_SECRET_ACCESS_KEY']\n",
    "output_data = \"S3_BUCKET_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "spark = SparkSession.builder\\\n",
    ".config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.5\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Datasets: \n",
    "* __us-cities-demographics__- Breakdown of the demographics of cities within the USA\n",
    "* __GlobalLandTemperaturesByCity__- Worldwide temperature dataset \n",
    "* __iata-codes.csv__- Iata code reference csv(source https://www.airportcodes.us/us-airports.htm)\n",
    "* __airport-codes_csv.csv__-Airport Dataset, containing airport and coordinate details\n",
    "* __18-83510-I94-Data-2016__- Immigration data for 2016 (Only April data is used in the project to show MVP without breaking AWS tier limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "ETL Process:\n",
    "\n",
    "The Datasets will be processed by Spark on an EMR instance, this will import the data from S3 and rewrite back to s3 in Parquet format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cities=spark.read.csv(\"us-cities-demographics.csv\", sep=';', header=True)\n",
    "df_temps=spark.read.csv(\"GlobalLandTemperaturesByCity.csv\", sep=',', header=True)\n",
    "df_iata=spark.read.csv(\"iata-codes.csv\", sep='-', header=True)#source https://www.airportcodes.us/us-airports.htm\n",
    "df_airports=spark.read.csv(\"airport-codes_csv.csv\", sep=',', header=True)\n",
    "df_immi =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data is run through the check_df() function to check which columns have nulls or duplicates, the columns with large amount of nulls are removed from the data, the data then is subsetted to get the columns we need for the final datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_df(df):\n",
    "    print('NaNs:\\n')\n",
    "    df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "    print('nulls:\\n')\n",
    "    df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    print( 'duplicates: ',df.count()-df.distinct().count())\n",
    "    print(df.count())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs:\n",
      "\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              0|                0|               0|                 0|           0|                     0|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n",
      "nulls:\n",
      "\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n",
      "duplicates:  0\n",
      "2891\n"
     ]
    }
   ],
   "source": [
    "check_df(df_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs:\n",
      "\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|                 0|                            0|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n",
      "nulls:\n",
      "\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|            364130|                       364130|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n",
      "duplicates:  0\n",
      "8599212\n"
     ]
    }
   ],
   "source": [
    "check_df(df_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs:\n",
      "\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|           0|        0|          0|         0|           0|       0|        0|         0|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "nulls:\n",
      "\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|        7006|        0|          0|         0|        5676|   14045|    45886|     26389|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n",
      "duplicates:  0\n",
      "55075\n"
     ]
    }
   ],
   "source": [
    "check_df(df_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs:\n",
      "\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|      0|      0|      0|     0|      0|    0|       0|       0|    0|      0|      0|      0|      0|      0|      0|     0|     0|      0|     0|    0|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n",
      "nulls:\n",
      "\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n",
      "duplicates:  0\n",
      "3096313\n"
     ]
    }
   ],
   "source": [
    "check_df(df_immi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The Data Model was chosen as it allows the immigration data to be analysed easily on a city by city basis, a monthly join from immigration to weather data especially allows the analyst to see the weather details for the month of arrival over previous years.\n",
    "\n",
    "\n",
    "The data was set up this way as it gives overall details of immigration throughout the US for easy summary reporting but also allowing the analyst to drill down to state and city level for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.1 model:\n",
    "#### dim_airport\n",
    " ident  \n",
    " type  \n",
    " name  \n",
    " elevation_ft  \n",
    " municipality  \n",
    " gps_code  \n",
    " iata_code   \n",
    " local_code  \n",
    " coordinates  \n",
    " \n",
    "#### dim_demographic\n",
    " City  \n",
    " State  \n",
    " Median Age  \n",
    " Male Population  \n",
    " Female Population  \n",
    " Total Population  \n",
    " Average Household Size  \n",
    " State Code  \n",
    " Race  \n",
    " Count  \n",
    "\n",
    "#### dim weather\n",
    " City  \n",
    " Country  \n",
    " Year  \n",
    " Month  \n",
    " AverageTemperature  \n",
    " Latitude  \n",
    " Longitude  \n",
    " \n",
    " \n",
    "#### fact_immigration\n",
    " cicid  \n",
    " i94mon  \n",
    " city  \n",
    " statecode  \n",
    " i94port  \n",
    " arrdate  \n",
    " i94mode  \n",
    " i94addr  \n",
    " depdate  \n",
    " i94bir  \n",
    " i94visa  \n",
    " visapost  \n",
    " entdepd  \n",
    " biryear  \n",
    " gender  \n",
    " airline  \n",
    " fltno  \n",
    " visatype  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![Relationship Diagram](IMMI_DB_DIA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_immigration(df_immi,df_iata):    \n",
    "    df_immi= df_immi.drop('occup','insnum', 'entdepu','i94yr', 'admnum','dtaddto','entdepa','matflag','entdepa','count','bdtadfile','dtadfile', 'i94cit','i94res')\n",
    "    df_immi= df_immi.join(df_iata,df_immi.i94port == df_iata.Code)\n",
    "    df_immi= df_immi.drop(df_immi.Code)\n",
    "    df_immi = df_immi.withColumn(\"biryear\", df_immi[\"biryear\"].cast(IntegerType()))\n",
    "    df_immi = df_immi.withColumn(\"i94mon\", df_immi[\"i94mon\"].cast(IntegerType()))\n",
    "    df_immi = df_immi.withColumn(\"cicid\", df_immi[\"cicid\"].cast(IntegerType()))\n",
    "    df_immi = df_immi.withColumn(\"arrdate\", df_immi[\"arrdate\"].cast(IntegerType()))\n",
    "    df_immi = df_immi.withColumn(\"arrdate\", date_adding_udf('arrdate'))\n",
    "    df_immi = df_immi.withColumn(\"depdate\", df_immi[\"depdate\"].cast(IntegerType()))\n",
    "    df_immi = df_immi.withColumn(\"depdate\", date_adding_udf('depdate'))\n",
    "    \n",
    "    return df_immi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def date_adding(days):\n",
    "        if(days):\n",
    "            date = dt.datetime(1960, 1, 1).date()\n",
    "            return (date + dt.timedelta(days)).isoformat()\n",
    "        return None\n",
    "\n",
    "date_adding_udf = udf(lambda z: date_adding(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_weather(df_temps):\n",
    "    df_temps=df_temps.filter(df_temps.AverageTemperature.isNotNull())\n",
    "    split_temp=split(df_temps['dt'], '-')     \n",
    "    df_temps= df_temps.withColumn('Year', split_temp.getItem(0))\n",
    "    df_temps= df_temps.withColumn('Month', split_temp.getItem(1))\n",
    "    df_temps = df_temps.withColumn('AverageTemperature' ,round(df_temps[\"AverageTemperature\"], 2))\n",
    "    df_temps = df_temps.select(\"City\",\"Country\",\"AverageTemperature\",\"Year\",\"Month\",\"Latitude\", \"Longitude\")\n",
    "    return df_temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_airports(df_airports):\n",
    "    df_airports = df_airports.where(\"iso_country = 'US' and iata_code is not null\")\n",
    "    df_airports = df_airports.select(\"ident\",\"type\",\"name\",\"elevation_ft\",\"municipality\",\"gps_code\",\"iata_code\",\"local_code\",\"coordinates\")\n",
    "    return df_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_demographics(df_cities):\n",
    "    df_cities1 = df_cities.selectExpr(\"monotonically_increasing_id() as Id\",\"*\")\n",
    "    df_cities = df_cities1\n",
    "    for col in df_cities1.columns:\n",
    "      df_cities = df_cities.withColumnRenamed(col,col.replace(\" \", \"_\"))\n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----------+-------+-------+----------+------+-------+--------+-------+-------+------+-------+-----+--------+-------+-----+\n",
      "|cicid|i94mon|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|visapost|entdepd|biryear|gender|airline|fltno|visatype|   City|State|\n",
      "+-----+------+-------+----------+-------+-------+----------+------+-------+--------+-------+-------+------+-------+-----+--------+-------+-----+\n",
      "|    7|     4|    ATL|2016-04-07|    1.0|     AL|      null|  25.0|    3.0|     SEO|   null|   1991|     M|   null|00296|      F1|Atlanta|   GA|\n",
      "|   27|     4|    BOS|2016-04-01|    1.0|     MA|2016-04-05|  58.0|    1.0|     TIA|      O|   1958|     M|     LH|00422|      B1| Boston|   MA|\n",
      "|   28|     4|    ATL|2016-04-01|    1.0|     MA|2016-04-05|  56.0|    1.0|     TIA|      O|   1960|     F|     LH|00422|      B1|Atlanta|   GA|\n",
      "|   29|     4|    ATL|2016-04-01|    1.0|     MA|2016-04-17|  62.0|    2.0|     TIA|      O|   1954|     M|     AZ|00614|      B2|Atlanta|   GA|\n",
      "|   30|     4|    ATL|2016-04-01|    1.0|     NJ|2016-05-04|  49.0|    2.0|     TIA|      O|   1967|     M|     OS|00089|      B2|Atlanta|   GA|\n",
      "+-----+------+-------+----------+-------+-------+----------+------+-------+--------+-------+-------+------+-------+-----+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi = clean_immigration(df_immi,df_iata)\n",
    "immi.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+----+-----+--------+---------+\n",
      "| City|Country|AverageTemperature|Year|Month|Latitude|Longitude|\n",
      "+-----+-------+------------------+----+-----+--------+---------+\n",
      "|Århus|Denmark|              6.07|1743|   11|  57.05N|   10.33E|\n",
      "|Århus|Denmark|              5.79|1744|   04|  57.05N|   10.33E|\n",
      "|Århus|Denmark|             10.64|1744|   05|  57.05N|   10.33E|\n",
      "|Århus|Denmark|             14.05|1744|   06|  57.05N|   10.33E|\n",
      "|Århus|Denmark|             16.08|1744|   07|  57.05N|   10.33E|\n",
      "+-----+-------+------------------+----+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temps = clean_weather(df_temps)\n",
    "temps.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|\n",
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = clean_airports(df_airports)\n",
    "airports.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "| Id|            City|        State|Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign-born|Average_Household_Size|State_Code|                Race|Count|\n",
      "+---+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|  0|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|  1|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|  2|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|  3|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|  4|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+---+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics = clean_demographics(df_cities)\n",
    "demographics.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4.2 Data Quality Checks\n",
    "Final datasets are checked in a few ways:\n",
    " * Count row checks on every table to ensure no bugs happened in load\n",
    " * Test Fact Primary key for nulls \n",
    " * Test Fact Primary key for duplicates\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Primary Key nulls in Fact Table:  0 , Count of Primary Key Duplicates in Fact Table:  0\n",
      "Table Row Counts: Temperatures  8235082 , Airports  2019 , Demographics  2891 , Immigration  1069528\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "#check primary id of fact table for nulls and duplicates\n",
    "immi.createOrReplaceTempView(\"immi_view\")\n",
    "fact_nulls=spark.sql(\"\"\"SELECT COUNT(*) as count FROM immi_view WHERE cicid IS NULL\"\"\")\n",
    "fact_duplicates=spark.sql(\"\"\"select sum(*) as sum from (SELECT COUNT(cicid) FROM immi_view group by cicid HAVING COUNT(*) > 1) as A\"\"\")\n",
    "null_count = fact_nulls.first()['count']\n",
    "dupe_count= fact_duplicates.first()['sum']\n",
    "if(dupe_count is None):\n",
    "    dupe_count = 0\n",
    "\n",
    "\n",
    "print('Count of Primary Key nulls in Fact Table: ',null_count,', Count of Primary Key Duplicates in Fact Table: ',dupe_count)\n",
    "a = temps.count()\n",
    "b = airports.count()\n",
    "c = demographics.count()\n",
    "d = immi.count()\n",
    "print('Table Row Counts:','Temperatures ',a,', Airports ',b,', Demographics ' ,c,', Immigration ',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports.write.mode(\"overwrite\").parquet(output_data +\"Airports\")\n",
    "temps.write.partitionBy(\"Month\").mode(\"overwrite\").parquet(output_data +\"Temperatures\")\n",
    "immi.write.mode(\"overwrite\").parquet(output_data +\"Immigration\")\n",
    "demographics.write.mode(\"overwrite\").parquet(output_data +\"Demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Attached Seperately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Tools used:\n",
    " * Amazon S3- Used for storage of initial datasets and final parquets- Chosen due to ease of access for all technologies and pricing scheme works well for data that isnt loaded in and out often.\n",
    " * Amazon EMR - Used to launch spark instance to run main.py, Running spark locally and via workspace is much slower and EMR is very simple to set up for repliucation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Run steps:\n",
    "    1.)Create amazon EMR cluster with hadoop and spark.\n",
    "    2.)SSH into the cluster\n",
    "    3.)SCP main.py and d1.cfg into cluster storage\n",
    "    4.)Run via command:\n",
    "        /usr/bin/spark-submit --packages saurfang:spark-sas7bdat:2.0.0-s_2.10 --master yarn ./main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Please see main.py for more information on the functions used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5 Different Scenario's\n",
    "#### The data was increased by 100x.\n",
    " * Scale up the EMR cluster to allow for higher use\n",
    " * Move data to Redshift/Snowflake/Cassandra or another data warehouse as that will be more efficient when accessing large amounts of data\n",
    " \n",
    " #### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * Set up a scheduler to run the job on the daily, Airflow or Cron job to trigger main.py\n",
    " \n",
    "  #### The database needed to be accessed by 100+ people.\n",
    " * Set up Redshift, you can use concurrency scaling (https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html) to allow as many users as needed access to the data, this however will scale up the price quite quickly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "References:\n",
    "\n",
    "\n",
    "    https://knowledge.udacity.com/\n",
    "    https://stackoverflow.com/questions/51949414/read-sas-sas7bdat-data-with-spark\n",
    "    https://www.airportcodes.us/us-airports.htm\n",
    "    https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data/notebooks\n",
    "    https://www.bmc.com/blogs/how-to-write-spark-udf-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
